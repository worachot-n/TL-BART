{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\worac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pprint\n",
    "import logging\n",
    "\n",
    "import datasets\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from filelock import FileLock\n",
    "from transformers import AdamW, get_scheduler, set_seed\n",
    "\n",
    "from transformers.file_utils import is_offline_mode\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "# from args import parse_args\n",
    "# from data_loader import raw_data_loader, data_processor\n",
    "# from model_loader import model_loader\n",
    "from rouge_s import py_rouge_scores\n",
    "from utils import label_smoothed_nll_loss, postprocess_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    MODEL_MAPPING,\n",
    "    SchedulerType,\n",
    ")\n",
    "\n",
    "# You should update this to your particular problem to have better documentation of `model_type`\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--debug'], dest='debug', nargs=0, const=True, default=False, type=None, choices=None, required=False, help='Use the debug mode or not', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "arg_parser = argparse.ArgumentParser(description=\"BART\")\n",
    "arg_parser.add_argument(\"--len_input\", dest=\"len_input\", type=str, default=None, help=\"Use the ctrlen model or not\", choices=('no', 'real', 'predict', 'surface'))\n",
    "arg_parser.add_argument(\"--len_output\", dest=\"len_output\", default=None, help=\"Use the ctrlen model or not\", choices=('no', 'real'))\n",
    "arg_parser.add_argument(\"--output_dir\", dest=\"output_dir\", type=str, default=\"./output/1\", help=\"default\")\n",
    "arg_parser.add_argument(\"--train_file\", dest=\"train_file\", type=str, default=None, help=\"A csv or a json file containing the training data.\")\n",
    "arg_parser.add_argument(\"--validation_file\", dest=\"validation_file\", type=str, default=None, help=\"A csv or a json file containing the validation data.\")\n",
    "arg_parser.add_argument(\"--test_file\", dest=\"test_file\", type=str, default=None, help=\"A csv or a json file containing the test data.\")\n",
    "arg_parser.add_argument(\"--ignore_pad_token_for_loss\", dest=\"ignore_pad_token_for_loss\", type=bool, default=True, help=\"Whether to ignore the tokens corresponding to \" \"padded labels in the loss computation or not.\",)\n",
    "arg_parser.add_argument(\"--text_column\", dest=\"text_column\", type=str, default=\"dialogue\", help=\"The name of the column in the datasets containing the full texts (for summarization).\")\n",
    "arg_parser.add_argument(\"--summary_column\", dest=\"summary_column\", type=str, default=\"summary\", help=\"The name of the column in the datasets containing the summaries (for summarization).\")\n",
    "arg_parser.add_argument(\"--model_name_or_path\", dest=\"model_name_or_path\", type=str, default=\"facebook/bart-large\", help=\"Path to pretrained model or model identifier from huggingface.co/models.\")\n",
    "arg_parser.add_argument(\"--model_type\", dest=\"model_type\", type=str, default=\"bart\", help=\"Model type to use if training from scratch.\", choices=MODEL_TYPES)\n",
    "arg_parser.add_argument(\"--max_source_length\", dest=\"max_source_length\", type=int, default=1024, help=\"default\")\n",
    "arg_parser.add_argument(\"--source_prefix\", dest=\"source_prefix\", type=str, default=None, help=\"A prefix to add before every source text \" \"(useful for T5 models).\")\n",
    "arg_parser.add_argument(\"--preprocessing_num_workers\", type=int, default=None, help=\"The number of processes to use for the preprocessing.\")\n",
    "# arg_parser.add_argument(\"--overwrite_cache\", dest=\"overwrite_cache\", type=lambda x:bool(strtobool(x)), default=True, help=\"default\")\n",
    "arg_parser.add_argument(\"--overwrite_cache\", dest=\"overwrite_cache\", type=bool, default=None, help=\"Overwrite the cached training and evaluation sets\")\n",
    "arg_parser.add_argument(\"--min_target_length\", dest=\"min_target_length\", type=int, default=1, help=\"The minimal total sequence length for target text\")\n",
    "arg_parser.add_argument(\"--max_target_length\", dest=\"max_target_length\", type=int, default=128, help=\"The maximum total sequence length for target text after \"\n",
    "        \"tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "        \"during ``evaluate`` and ``predict``.\")\n",
    "arg_parser.add_argument(\"--num_beams\", dest=\"num_beams\", type=int, default=4, help=\"Number of beams to use for evaluation. This argument will be \"\n",
    "        \"passed to ``model.generate``, which is used during ``evaluate`` and ``predict``.\")\n",
    "arg_parser.add_argument(\"--learning_rate\", dest=\"learning_rate\", type=float, default=5e-5, help=\"Initial learning rate (after the potential warmup period) to use.\")\n",
    "arg_parser.add_argument(\"--pad_to_max_length\", action=\"store_true\", help=\"If passed, pad all samples to `max_length`. Otherwise, dynamic padding is used.\",)\n",
    "arg_parser.add_argument(\"--weight_decay\", dest=\"weight_decay\", type=float, default=1e-3, help=\"Weight decay to use.\")\n",
    "arg_parser.add_argument(\"--label_smoothing\", dest=\"label_smoothing\", type=float, default=0.1, help=\"hyperparameter for label smoothing.\")\n",
    "arg_parser.add_argument(\"--length_penalty\", dest=\"length_penalty\", type=float, default=1.0, help=\"large - longer sequence, small - shorter sequence\")\n",
    "arg_parser.add_argument(\"--num_train_epochs\", dest=\"num_train_epochs\", type=int, default=15, help=\"Total number of training epochs to perform.\")\n",
    "arg_parser.add_argument(\"--per_device_train_batch_size\", dest=\"per_device_train_batch_size\", type=int, default=8, help=\"Batch size (per device) for the training dataloader.\")\n",
    "arg_parser.add_argument(\"--gradient_accumulation_steps\", dest=\"gradient_accumulation_steps\", type=int, default=64, help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "arg_parser.add_argument(\"--per_device_eval_batch_size\", dest=\"per_device_eval_batch_size\", type=int, default=8, help=\"Batch size (per device) for the evaluation dataloader.\")\n",
    "arg_parser.add_argument(\"--per_device_test_batch_size\", dest=\"per_device_test_batch_size\", type=int, default=8, help=\"Batch size (per device) for the evaluation dataloader.\")\n",
    "arg_parser.add_argument(\"--num_warmup_steps\", dest=\"num_warmup_steps\", type=int, default=0, help=\"Number of steps for the warmup in the lr scheduler.\")\n",
    "arg_parser.add_argument(\"--cache_dir\", dest=\"cache_dir\", type=str, default=\"./output/cache\", help=\"default\")\n",
    "arg_parser.add_argument(\"--seed\", dest=\"seed\", type=int, default=12345, help=\"default\")\n",
    "# arg_parser.add_argument(\"-f\", required=False) #important\n",
    "arg_parser.add_argument(\"--config_name\", type=str, default=None, help=\"Pretrained config name or path if not the same as model_name\")\n",
    "arg_parser.add_argument(\"--tokenizer_name\", type=str, default=None, help=\"Pretrained tokenizer name or path if not the same as model_name\")\n",
    "arg_parser.add_argument(\"--use_slow_tokenizer\", dest=\"use_slow_tokenizer\", action=\"store_true\", help=\"If passed, will use a slow tokenizer (not backed by the ðŸ¤— Tokenizers library).\")\n",
    "arg_parser.add_argument(\"--max_train_steps\", type=int, default=None, help=\"Total number of training steps to perform. If provided, overrides num_train_epochs.\")\n",
    "arg_parser.add_argument(\"--lr_scheduler_type\", type=SchedulerType, default=\"linear\", help=\"The scheduler type to use.\", choices=[\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"])\n",
    "arg_parser.add_argument(\"--ctrlen_model\", action='store_true', default=False, help=\"Use the ctrlen model or not\")\n",
    "arg_parser.add_argument(\"--sim_window_size\", type=int, default=5, help=\"window size for computing loss.\")\n",
    "arg_parser.add_argument(\"--sim_loss\", type=float, default=0, help=\"the loss weight for similarity scores.\")\n",
    "arg_parser.add_argument(\"--special_len_token_init\", type=str, default=None, help=\"ways to initialize special token for length (random, zero, token_embs)\")\n",
    "arg_parser.add_argument(\"--embedding_lr\", type=float, default=5e-5, help=\"Initial learning rate for embedding layers.\")\n",
    "arg_parser.add_argument(\"--len_start\", type=int, default=1, help=\"start length.\")\n",
    "arg_parser.add_argument(\"--len_end\", type=int, default=100, help=\"end length.\")\n",
    "arg_parser.add_argument(\"--data_aug\",action='store_true',default=False,help=\"whether to perform data augmentation or not\")\n",
    "arg_parser.add_argument(\"--pred_len\", action='store_true', default=False, help=\"whether to use the golden length or predicted length\")\n",
    "arg_parser.add_argument(\"--shuffle\", action='store_true', default=False, help=\"whether to shuffle the dataset to balance train/validation/test\")\n",
    "arg_parser.add_argument(\"--debug\", action='store_true', default=False, help=\"Use the debug mode or not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = arg_parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.len_input = 'no'\n",
    "args.len_output = 'no'\n",
    "args.output_dir = \"./output/1\"\n",
    "args.train_file = \"./data/dialogsum/dialogsum.train.jsonl\"\n",
    "args.validation_file = \"./data/dialogsum/dialogsum.dev.jsonl\"\n",
    "args.test_file = \"./data/dialogsum/dialogsum.test.jsonl\"\n",
    "args.text_column = \"dialogue\"\n",
    "args.summary_column = \"summary\"\n",
    "args.model_name_or_path = \"facebook/bart-large\"\n",
    "args.model_type = \"bart\"\n",
    "args.max_source_length = 1024\n",
    "args.min_target_length = 1\n",
    "args.max_target_length = 128\n",
    "args.num_beams = 4\n",
    "args.learning_rate = 5e-5\n",
    "args.weight_decay = 1e-3\n",
    "args.label_smoothing = 0.1\n",
    "args.length_penalty = 1.0 \n",
    "args.num_train_epochs = 15 \n",
    "args.per_device_train_batch_size = 2 \n",
    "args.gradient_accumulation_steps = 64 \n",
    "args.per_device_eval_batch_size = 8 \n",
    "args.per_device_test_batch_size = 8 \n",
    "args.num_warmup_steps = 0 \n",
    "args.cache_dir = \"./output/cache\"\n",
    "args.overwrite_cache = True\n",
    "args.seed = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import utils\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "def load_from_dialogsum(args, file_path):\n",
    "    ''' load dialoguesum jsonl data '''\n",
    "\n",
    "    data = []\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    \n",
    "    id_list       = [sample['fname'] for sample in data]\n",
    "    dialogue_list = [sample['dialogue'] for sample in data]\n",
    "\n",
    "    if 'summary' in data[0]:\n",
    "        #summary\n",
    "        summary_list  = [sample['summary'] for sample in data]\n",
    "        #topic\n",
    "        topic_list = [sample['topic'] for sample in data]\n",
    "\n",
    "    elif 'summary1' in data[0]:\n",
    "\n",
    "        id_list1 = [id+\"_sum1\" for id in id_list]\n",
    "        id_list2 = [id+\"_sum2\" for id in id_list]\n",
    "        id_list3 = [id+\"_sum3\" for id in id_list]\n",
    "\n",
    "        id_list = id_list1 + id_list2 + id_list3\n",
    "        dialogue_list = dialogue_list + dialogue_list + dialogue_list\n",
    "\n",
    "        #summary\n",
    "        summary_list1  = [sample['summary1'] for sample in data]\n",
    "        summary_list2  = [sample['summary2'] for sample in data]\n",
    "        summary_list3  = [sample['summary3'] for sample in data]\n",
    "\n",
    "        summary_list = summary_list1 + summary_list2 + summary_list3\n",
    "        \n",
    "        #topic\n",
    "        topic_list1  = [sample['topic1'] for sample in data]\n",
    "        topic_list2  = [sample['topic2'] for sample in data]\n",
    "        topic_list3  = [sample['topic3'] for sample in data]\n",
    "        \n",
    "        topic_list = topic_list1 + topic_list2 + topic_list3\n",
    "\n",
    "    data_dict = {'id': id_list,\n",
    "                'dialogue': dialogue_list,\n",
    "                'summary': summary_list,\n",
    "                'topic': topic_list}\n",
    "\n",
    "    data_dict = Dataset.from_dict(data_dict)\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "========================================================================================================================================================================================================\n",
      "ID:       test_0_sum1\n",
      "summary:  Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
      "topic:    communication method  \n",
      "ID:       test_0_sum2\n",
      "summary:  In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n",
      "topic:    company policy  \n",
      "ID:       test_0_sum3\n",
      "summary:  Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.\n",
      "topic:    dictation  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dialogsum_test = load_from_dialogsum(args, \"./data/dialogsum/dialogsum.test.jsonl\")\n",
    "# print sample data\n",
    "num_sample = 0\n",
    "print(f\"\"\"\n",
    "Dialogue:\\n{dialogsum_test['dialogue'][num_sample]}\n",
    "{\"=\"*200}\n",
    "ID:       {dialogsum_test['id'][num_sample]}\n",
    "summary:  {dialogsum_test['summary'][num_sample]}\n",
    "topic:    {dialogsum_test['topic'][num_sample]}  \n",
    "ID:       {dialogsum_test['id'][num_sample+500]}\n",
    "summary:  {dialogsum_test['summary'][num_sample+500]}\n",
    "topic:    {dialogsum_test['topic'][num_sample+500]}  \n",
    "ID:       {dialogsum_test['id'][num_sample+1000]}\n",
    "summary:  {dialogsum_test['summary'][num_sample+1000]}\n",
    "topic:    {dialogsum_test['topic'][num_sample+1000]}  \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_data_loader(args):\n",
    "    ''' load raw datasets from csv files '''\n",
    "\n",
    "    data_files = {}\n",
    "    if args.train_file is not None:\n",
    "        data_files[\"train\"] = args.train_file\n",
    "    if args.validation_file is not None:\n",
    "        data_files[\"validation\"] = args.validation_file\n",
    "    if args.test_file is not None:\n",
    "        data_files[\"test\"] = args.test_file\n",
    "\n",
    "    if 'dialogsum' in args.train_file:\n",
    "        train_dict = load_from_dialogsum(args, args.train_file)\n",
    "        val_dict   = load_from_dialogsum(args, args.validation_file)\n",
    "        test_dict  = load_from_dialogsum(args, args.test_file)\n",
    "\n",
    "    train_dict = utils.len_adjust(args, train_dict, 'train')\n",
    "    val_dict   = utils.len_adjust(args, val_dict, 'val')\n",
    "    test_dict  = utils.len_adjust(args, test_dict, 'test')\n",
    "\n",
    "    raw_datasets = datasets.DatasetDict({\"train\":train_dict, \"validation\":val_dict, \"test\":test_dict})\n",
    "\n",
    "    return raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "========================================================================================================================================================================================================\n",
      "ID:       test_0_sum1\n",
      "summary:  Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
      "topic:    communication method  \n",
      "ID:       test_0_sum2\n",
      "summary:  In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n",
      "topic:    company policy  \n",
      "ID:       test_0_sum3\n",
      "summary:  Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.\n",
      "topic:    dictation  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dialogsum = raw_data_loader(args)\n",
    "# print sample data\n",
    "num_sample = 0\n",
    "print(f\"\"\"\n",
    "Dialogue:\\n{dialogsum['test']['dialogue'][num_sample]}\n",
    "{\"=\"*200}\n",
    "ID:       {dialogsum['test']['id'][num_sample]}\n",
    "summary:  {dialogsum['test']['summary'][num_sample]}\n",
    "topic:    {dialogsum['test']['topic'][num_sample]}  \n",
    "ID:       {dialogsum['test']['id'][num_sample+500]}\n",
    "summary:  {dialogsum['test']['summary'][num_sample+500]}\n",
    "topic:    {dialogsum['test']['topic'][num_sample+500]}  \n",
    "ID:       {dialogsum['test']['id'][num_sample+1000]}\n",
    "summary:  {dialogsum['test']['summary'][num_sample+1000]}\n",
    "topic:    {dialogsum['test']['topic'][num_sample+1000]}  \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processor(logger, args, accelerator, raw_datasets, tokenizer, model):\n",
    "    ''' prepare dataset format for train/val/test '''\n",
    "    def preprocess_function(examples):\n",
    "\n",
    "        # summary - target\n",
    "        targets = examples[summary_column]\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "        if args.ctrlen_model:\n",
    "            gold_sum_len = [len(item) for item in labels['attention_mask']]\n",
    "\n",
    "        # dialogue - input\n",
    "        inputs = examples[text_column]\n",
    "        new_inputs = []\n",
    "        for i, inp in enumerate(inputs):\n",
    "            if args.ctrlen_model:\n",
    "                if 'pred_len' in examples:\n",
    "                    new_inputs.append(prefix + \"<len_{}> \".format(examples['pred_len'][i]) + inp)\n",
    "\n",
    "                else:\n",
    "                    new_inputs.append(prefix + \"<len_{}> \".format(gold_sum_len[i]) + inp)\n",
    "            else:\n",
    "                new_inputs.append(prefix + inp)\n",
    "\n",
    "        inputs = new_inputs\n",
    "        model_inputs = tokenizer(inputs, max_length=args.max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "        # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "        # padding in the loss.\n",
    "        if padding == \"max_length\" and args.ignore_pad_token_for_loss:\n",
    "            labels[\"input_ids\"] = [\n",
    "                [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "            ]\n",
    "        \n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "        if args.ctrlen_model:\n",
    "            model_inputs[\"gold_len\"] = gold_sum_len\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    prefix = args.source_prefix if args.source_prefix is not None else \"\"\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # First we tokenize all the texts.\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "\n",
    "    # Get the column names for input/target.\n",
    "    text_column = args.text_column\n",
    "    if text_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--text_column' value '{args.text_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n",
    "\n",
    "    summary_column = args.summary_column\n",
    "    if summary_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--summary_column' value '{args.summary_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n",
    "\n",
    "    # Temporarily set max_target_length for training.\n",
    "    max_target_length = args.max_target_length\n",
    "    padding = \"max_length\" if args.pad_to_max_length else False\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        processed_datasets = raw_datasets.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            batch_size=1000,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n",
    "\n",
    "    train_dataset = processed_datasets[\"train\"]\n",
    "    eval_dataset  = processed_datasets[\"validation\"]\n",
    "    test_dataset  = processed_datasets[\"test\"]\n",
    "\n",
    "    # Log a few random samples from the training set:\n",
    "    for index in random.sample(range(len(train_dataset)), 1):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "    label_pad_token_id = -100 if args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "        pad_to_multiple_of=8 if accelerator.use_fp16 else None,\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\n",
    "    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n",
    "    test_dataloader = DataLoader(test_dataset, collate_fn=data_collator, batch_size=args.per_device_test_batch_size)\n",
    "\n",
    "    return (train_dataloader, eval_dataloader, test_dataloader), (train_dataset, eval_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/20/2023 12:28:41 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cpu\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accelerator = Accelerator(fp16=True)\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "logger.info(accelerator.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator.is_local_main_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.utils.logging.set_verbosity_warning()\n",
    "transformers.utils.logging.set_verbosity_info()\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = False \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator.is_main_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(args.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config, tokenizer, model = model_loader(accelerator, logger, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, processed_dataset = data_processor(logger, args, accelerator, raw_datasets, tokenizer, model)\n",
    "train_dataloader, eval_dataloader, test_dataloader = dataloader\n",
    "train_dataset, _, _ = processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = Training Preparation = = =\n",
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "if args.ctrlen_model: \n",
    "    no_decay_emb_matrix = [\"bias\", \"LayerNorm.weight\", \"shared\"]\n",
    "else:\n",
    "    no_decay_emb_matrix = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay_emb_matrix)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "if args.ctrlen_model:\n",
    "    if args.model_type == 'bart': \n",
    "        optimizer_grouped_parameters.extend([{\n",
    "            \"params\": model.seq2seq_model.model.shared.parameters(),\n",
    "            \"lr\": args.embedding_lr}])\n",
    "    elif args.model_type == 't5':\n",
    "        optimizer_grouped_parameters.extend([{\n",
    "            \"params\": model.seq2seq_model.shared.parameters(),\n",
    "            \"lr\": args.embedding_lr}])\n",
    "    else:\n",
    "        raise ValueError('{} model type not implemented'.format(args.model_type))\n",
    "\n",
    "# optimizer\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "model, optimizer, train_dataloader, eval_dataloader, test_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, test_dataloader\n",
    ")\n",
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "else:\n",
    "    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.num_warmup_steps,\n",
    "    num_training_steps=args.max_train_steps,\n",
    ")\n",
    "\n",
    "# = = = = = = = = = = = = = = = = Train = = = = = = = = = = = = = = = = = = =\n",
    "total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\" Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\" Num Epochs = {args.num_train_epochs}\")\n",
    "logger.info(f\" Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "logger.info(f\" Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\" Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\" Total optimization steps = {args.max_train_steps}\")\n",
    "\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(args.max_train_steps), desc=\"Training: \", disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "\n",
    "val_results = []\n",
    "acc_losses  = []\n",
    "best_r2_f1  = None\n",
    "best_epoch  = 0\n",
    "\n",
    "if args.model_type == 'bart' or args.model_type == 't5':\n",
    "    task_specific_params = model.config.task_specific_params\n",
    "    params = task_specific_params.get('summarization', {})\n",
    "    params['min_length'] = args.min_target_length\n",
    "    params['max_length'] = args.max_target_length\n",
    "    params['length_penalty'] = args.length_penalty\n",
    "    params['num_beams'] = args.num_beams\n",
    "    model.config.update(params)\n",
    "else:\n",
    "    raise ValueError('{} model type not implemented'.format(args.model_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  = Train =  =  =  =  =  =  =  =  =  =  =  =  =  =  = \n",
    "for epoch in range(args.num_train_epochs):\n",
    "    # train\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if args.ctrlen_model: # CTRLen model\n",
    "            outputs, loss = model(batch, tokenizer)\n",
    "        else: # w/ and w/o label smoothing (always better with label smoothing)\n",
    "            if args.label_smoothing == 0:\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "            else:\n",
    "                outputs = model(**batch)\n",
    "                output_logits = outputs.logits\n",
    "                output_probs = torch.nn.functional.log_softmax(output_logits, dim=-1)\n",
    "                output_probs = output_probs.view(-1, model.config.vocab_size)\n",
    "\n",
    "                gt_logits = batch['labels']\n",
    "                gt_logits = gt_logits.view(-1)\n",
    "\n",
    "                loss, _ = label_smoothed_nll_loss(output_probs, gt_logits, args.label_smoothing, ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "        acc_losses.append(loss.item())\n",
    "        loss = loss / args.gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix(lr=lr_scheduler.get_last_lr()[0], loss=np.mean(acc_losses[-50:]))\n",
    "            completed_steps += 1\n",
    "\n",
    "        if completed_steps >= args.max_train_steps:\n",
    "            break\n",
    "\n",
    "    # =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  = EVAL =  =  =  =  =  =  =  =  =  =  =  =  =  =  = \n",
    "    model.eval()\n",
    "    val_predict     = []\n",
    "    val_groundtruth = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "            labels = batch[\"labels\"]\n",
    "            if not args.pad_to_max_length:\n",
    "                # If we did not pad to max length, we need to pad the labels too\n",
    "                labels = accelerator.pad_across_processes(batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n",
    "\n",
    "            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "            labels = accelerator.gather(labels).cpu().numpy()\n",
    "\n",
    "            if args.ignore_pad_token_for_loss:\n",
    "                # Replace -100 in the labels as we can't decode them.\n",
    "                labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "\n",
    "            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "            val_predict.extend(decoded_preds)\n",
    "            val_groundtruth.extend(decoded_labels)\n",
    "\n",
    "    if args.len_output == 'real':\n",
    "        new_val_predict = []\n",
    "        for sample in val_predict:\n",
    "            try:\n",
    "                gen_sum = sample.split('Summary: ')[2]\n",
    "                new_val_predict.append(gen_sum)\n",
    "            except:\n",
    "                new_val_predict.append(sample)\n",
    "        val_predict = new_val_predict\n",
    "    else:\n",
    "        new_val_predict = val_predict\n",
    "\n",
    "    logger.info(\"\")\n",
    "    logger.info(\"Rouge score on val set after epoch {}\".format(epoch+1))\n",
    "    eval_results = py_rouge_scores(val_predict, val_groundtruth)\n",
    "\n",
    "    if best_r2_f1 is None:\n",
    "        best_r2_f1 = eval_results\n",
    "    if eval_results['rouge-2']['f'] >= best_r2_f1['rouge-2']['f']:\n",
    "        best_r2_f1 = eval_results\n",
    "        best_epoch = epoch + 1\n",
    "\n",
    "        os.makedirs(args.output_dir+'/best', exist_ok=True)\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(args.output_dir+'/best', save_function=accelerator.save)\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(args.output_dir+'/best')\n",
    "\n",
    "        # save vocab\n",
    "        vocab = tokenizer.vocab.copy()\n",
    "        vocab = {k: v for k, v in sorted(vocab.items(), key=lambda item: item[1])}\n",
    "        with open(args.output_dir + '/best/vocab.txt', 'w') as f:\n",
    "            for word, index in vocab.items():\n",
    "                # it lead to encoding bug on some machines, so i add this line\n",
    "                word = word.encode('ascii', 'ignore').decode('ascii')\n",
    "                f.write(str(index) + ': ' + word + '\\n')\n",
    "\n",
    "    # = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "    logger.info(\"Current Best Validation Result is at epoch {}\".format(best_epoch))\n",
    "    py_rouge_scores(None, None, best_r2_f1)\n",
    "\n",
    "\n",
    "# =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  = Test =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  = \n",
    "# load best model\n",
    "logger.info(\"Loading Best Result is at epoch {} for Testing\".format(best_epoch))\n",
    "\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "config          = config.from_pretrained(args.output_dir+'/best')\n",
    "tokenizer       = tokenizer.from_pretrained(args.output_dir+'/best', config=config)\n",
    "unwrapped_model = unwrapped_model.from_pretrained(args.output_dir+'/best', config=config)\n",
    "model           = accelerator.prepare(unwrapped_model)\n",
    "\n",
    "if args.model_type == 'bart' or args.model_type == 't5':\n",
    "    task_specific_params = model.config.task_specific_params\n",
    "    params = task_specific_params.get('summarization', {})\n",
    "    params['min_length'] = args.min_target_length\n",
    "    params['max_length'] = args.max_target_length\n",
    "    params['length_penalty'] = args.length_penalty\n",
    "    params['num_beams'] = args.num_beams\n",
    "    model.config.update(params)\n",
    "else:\n",
    "    raise ValueError('{} model type not implemented'.format(args.model_type))\n",
    "\n",
    "# start Test \n",
    "logger.info(\"Collecting Testing Result...\")\n",
    "model.eval()\n",
    "\n",
    "test_predict     = []\n",
    "test_groundtruth = []\n",
    "for step, batch in enumerate(tqdm(test_dataloader, leave=False)):\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "        generated_tokens = accelerator.pad_across_processes(\n",
    "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        if not args.pad_to_max_length:\n",
    "            # If we did not pad to max length, we need to pad the labels too\n",
    "            labels = accelerator.pad_across_processes(batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n",
    "\n",
    "        generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "        labels = accelerator.gather(labels).cpu().numpy()\n",
    "\n",
    "        if args.ignore_pad_token_for_loss:\n",
    "            # Replace -100 in the labels as we can't decode them.\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        if isinstance(generated_tokens, tuple):\n",
    "            generated_tokens = generated_tokens[0]\n",
    "\n",
    "        decoded_preds  = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "        decoded_preds  = [' '.join(sent.split('\\n')) for sent in decoded_preds]\n",
    "        decoded_labels = [' '.join(sent.split('\\n')) for sent in decoded_labels]\n",
    "\n",
    "        test_predict.extend(decoded_preds)\n",
    "        test_groundtruth.extend(decoded_labels)\n",
    "\n",
    "print(raw_datasets['test']['dialogue'][0])\n",
    "\n",
    "if args.len_output == 'real':\n",
    "    new_test_predict = []\n",
    "    for sample in test_predict:\n",
    "        try:\n",
    "            gen_sum = sample.split('Summary: ')[2]\n",
    "            new_test_predict.append(gen_sum)\n",
    "        except:\n",
    "            new_test_predict.append(sample)\n",
    "    test_predict = new_test_predict\n",
    "\n",
    "logger.info(\"\")\n",
    "logger.info(\"ROUGE score on test set\")\n",
    "test_scores = py_rouge_scores(test_predict, test_groundtruth)\n",
    "logger.info(\"\")\n",
    "\n",
    "\n",
    "# Save generated summaries\n",
    "if args.len_input == 'predict':\n",
    "    os.makedirs(args.output_dir+'/predict_gen_samples', exist_ok=True)\n",
    "else:\n",
    "    os.makedirs(args.output_dir+'/gen_samples', exist_ok=True)\n",
    "\n",
    "for i in range(len(test_predict)):\n",
    "    test_id        = raw_datasets['test']['id'][i]\n",
    "    test_dialogue  = raw_datasets['test']['dialogue'][i]\n",
    "    test_summary   = raw_datasets['test']['summary'][i]\n",
    "    test_predict_s = test_predict[i]\n",
    "\n",
    "    if args.len_input == 'predict':\n",
    "        with open(args.output_dir+'/predict_gen_samples/'+str(test_id)+'.txt', 'w') as f:\n",
    "            test_dialogue = test_dialogue.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_dialogue)\n",
    "            f.write('\\n\\n')\n",
    "            f.write('Golden Summary:\\n')\n",
    "            test_summary = test_summary.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_summary)\n",
    "            f.write('\\n\\n')\n",
    "            f.write('Generate Summary:\\n')\n",
    "            test_predict_s = test_predict_s.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_predict_s)\n",
    "    else:\n",
    "        with open(args.output_dir+'/gen_samples/'+str(test_id)+'.txt', 'w') as f:\n",
    "            test_dialogue = test_dialogue.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_dialogue)\n",
    "            f.write('\\n\\n')\n",
    "            f.write('Golden Summary:\\n')\n",
    "            test_summary = test_summary.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_summary)\n",
    "            f.write('\\n\\n')\n",
    "            f.write('Generate Summary:\\n')\n",
    "            test_predict_s = test_predict_s.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(test_predict_s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
